{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdFLEEedp9wPXHPOBgVgJb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/talhaahussain/grappling-pose-identification/blob/main/src/Pose_Estimation_for_Grappling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### // **Briefing**\n",
        "\n",
        "This is a notebook for identifying grappling positions, through computer vision and binary classification.\n",
        "\n",
        "This does away with any real-time capture and playback, instead opting for more reliable (and less system-intensive) persistent data retrieval and storage. Obtaining images and video, and loading them into the environment is **your** responsibility."
      ],
      "metadata": {
        "id": "uBb0UM7Fy3vQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### // **Setup**\n",
        "\n",
        "Please run the following to install a required library for computer vision."
      ],
      "metadata": {
        "id": "j_6mm96GokRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install super-gradients"
      ],
      "metadata": {
        "id": "Gm0p3l5Sfxe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**After running the above, please restart your runtime environment.**\n",
        "\n",
        "\n",
        "This must be done in order to avoid an issue with PIL, where `Image.open()` (used by the model to load images) fails and instead raises exception `PIL.UnidentifiedImageError`.\n",
        "\n",
        "You can restart your runtime environment by going to the \"Runtime\" section overhead, and clicking \"Restart Session\"."
      ],
      "metadata": {
        "id": "p2Gcma78GtT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the CSV dataset has been provided, the corresponding images have not. To obtain them, go to https://vicos.si/resources/jiujitsu/"
      ],
      "metadata": {
        "id": "Br-TA7Xz4FIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### // **Imports**\n",
        "\n",
        "Please run the following cell in order to import all necessary libraries and modules."
      ],
      "metadata": {
        "id": "FxcJZrrko8yY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import pathlib\n",
        "import re\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import cv2\n",
        "from super_gradients.training import models\n",
        "from super_gradients.common.object_names import Models\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "from sklearn.preprocessing import normalize\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ywtyj2QLpBVf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a23d966-1b8f-45ab-e379-c691d03a0d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The console stream is logged into /root/sg_logs/console.log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2024-05-01 09:35:14] INFO - crash_tips_setup.py - Crash tips is enabled. You can set your environment variable to CRASH_HANDLER=FALSE to disable it\n",
            "[2024-05-01 09:35:18] INFO - utils.py - NumExpr defaulting to 2 threads.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### // **Load Device**\n",
        "\n",
        "Please run the following to allow your device of choice (CPU or GPU) to be used.\n",
        "\n",
        "To enable GPU runtime, click \"Connect\" in the top right hand corner, and choose \"Change Runtime Type\"."
      ],
      "metadata": {
        "id": "XYgIY23NpOeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If GPU available, use it\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ],
      "metadata": {
        "id": "n92D1QS9pQhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### // **Setting up Classifier**\n",
        "\n",
        "Run all of the following cells in order to set up the MLP class and associated functions."
      ],
      "metadata": {
        "id": "e7brw2sQpeFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_data(file=\"pin_dataset.csv\", test_size=0.20, seed=50):\n",
        "  X = []\n",
        "  y = []\n",
        "  df = pd.read_csv(file)\n",
        "\n",
        "  y = df[\"pin\"].to_numpy()\n",
        "  pose1 = df[\"pose1\"].to_numpy()\n",
        "  pose2 = df[\"pose2\"].to_numpy()\n",
        "\n",
        "  # Populate features list with contents of dataframes\n",
        "  for i in range(len(pose1)):\n",
        "    X.append((np.array(ast.literal_eval(pose1[i]) + ast.literal_eval(pose2[i]))).flatten())\n",
        "\n",
        "  X = np.array(X)\n",
        "\n",
        "  # Apply min-max normalization to features\n",
        "  X = normalize(X, axis=0, norm='max')\n",
        "\n",
        "  # Split into train and test (holdout) datasets\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
        "\n",
        "  # Perform random oversampling on both train and test sets\n",
        "  oversample = RandomOverSampler(sampling_strategy='minority')\n",
        "  X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
        "  X_test, y_test = oversample.fit_resample(X_test, y_test)\n",
        "\n",
        "  return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "KQtpJYcGqDqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, n_features=3*17*2, n_hidden=34, n_classes=1):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(n_features, n_hidden)\n",
        "    self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
        "    self.fc3 = nn.Linear(n_hidden, n_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.sigmoid(self.fc3(x))\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "uH9izCCzqTCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(X_train, y_train, mlp, epochs=100, seed=50):\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "  X_train = torch.FloatTensor(X_train)\n",
        "  y_train = torch.LongTensor(y_train)\n",
        "\n",
        "  # Use Binary Cross-Entropy as loss function\n",
        "  criterion = nn.BCELoss()\n",
        "\n",
        "  # Use Adam as optimizer\n",
        "  optimizer = torch.optim.Adam(mlp.parameters(), lr=0.01)\n",
        "  losses = []\n",
        "  for i in range(epochs):\n",
        "    # Make prediction\n",
        "    y_pred = mlp.forward(X_train).squeeze(-1)\n",
        "\n",
        "    # Evaluate loss\n",
        "    loss = criterion(y_pred, y_train.float())\n",
        "    losses.append(loss.detach().numpy())\n",
        "\n",
        "    if i % 10 == 0:\n",
        "      print(f\"Epoch: {i}, loss: {loss}\")\n",
        "\n",
        "    # Backpropogate\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  return epochs, losses"
      ],
      "metadata": {
        "id": "dbSqOD-GqzKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(X_test, y_test, mlp):\n",
        "  X_test = torch.FloatTensor(X_test)\n",
        "  y_test = torch.LongTensor(y_test)\n",
        "\n",
        "  predicted = []\n",
        "  predicted_class = []\n",
        "  actual_class = []\n",
        "  correct = 0\n",
        "\n",
        "  # Turn off backpropogation training\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(X_test):\n",
        "      y_eval = mlp.forward(data)\n",
        "\n",
        "      #print(f\"{i+1}.) {str(y_eval)} \\t {y_test[i]}\")\n",
        "\n",
        "      if round(y_eval.item()) == y_test[i]:\n",
        "        correct += 1\n",
        "\n",
        "      predicted.append(y_eval.item())\n",
        "      predicted_class.append(round(y_eval.item()))\n",
        "      actual_class.append(y_test[i])\n",
        "\n",
        "  return correct, i, predicted, predicted_class, actual_class\n"
      ],
      "metadata": {
        "id": "BZqMB9_MsL_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, filename=\"mlp.pt\"):\n",
        "  torch.save(model.state_dict(), filename)\n",
        "  print(f\"Saved model to {filename}.\")\n"
      ],
      "metadata": {
        "id": "3VH6hi47q4yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(filename=\"mlp.pt\"):\n",
        "  mlp = MLP()\n",
        "  mlp.load_state_dict(torch.load(filename))\n",
        "  print(f\"Loaded model from {filename}.\")\n",
        "  return mlp"
      ],
      "metadata": {
        "id": "B2FWdnvZ6JCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell to create a MLP model, train it, test it, get its results and save it!"
      ],
      "metadata": {
        "id": "d11xnu-5WS-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### USE THIS CELL TO EXECUTE FUNCTIONS FOR THE CLASSIFIER!\n",
        "\n",
        "X_train, X_test, y_train, y_test = init_data()\n",
        "mlp = MLP()\n",
        "epochs, losses = train_model(X_train, y_train, mlp, epochs=1000)\n",
        "\n",
        "plt.plot(range(epochs), losses)\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "\n",
        "correct, total, predicted, predicted_class, actual_class = test_model(X_test, y_test, mlp)\n",
        "print(f\"Correct: {correct} out of {total} on unseen data.\")\n",
        "confusion_matrix = metrics.confusion_matrix(actual_class, predicted_class)\n",
        "\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1])\n",
        "\n",
        "cm_display.plot()\n",
        "plt.show()\n",
        "\n",
        "save_model(mlp)"
      ],
      "metadata": {
        "id": "5dbi8anqqOCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### // **Keypoint Detection**"
      ],
      "metadata": {
        "id": "R-_u1rhuBECc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_position(mlp, poses):\n",
        "  # if (people in image) != 2...\n",
        "  if len(poses) != 102:\n",
        "    return 0.0\n",
        "  poses = torch.FloatTensor(poses)\n",
        "  with torch.no_grad():\n",
        "    pred = mlp(poses)\n",
        "\n",
        "  return pred.item()\n"
      ],
      "metadata": {
        "id": "p_WE-RkJqt9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_HPE_model(device):\n",
        "  # Initialise machine learning model\n",
        "  yolo_nas_pose = models.get(\"yolo_nas_pose_l\", pretrained_weights=\"coco_pose\").to(device)\n",
        "  return yolo_nas_pose"
      ],
      "metadata": {
        "id": "gwBneV23B2N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image(HPE_model, input_path, confidence):\n",
        "  result = HPE_model.predict(input_path, conf=confidence)\n",
        "  return result"
      ],
      "metadata": {
        "id": "6UNWyLV4B5P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_poses(result):\n",
        "  poses = result.prediction.poses\n",
        "  landmarks = np.array([])\n",
        "  for person, pose in enumerate(poses):\n",
        "    landmarks = np.concatenate((landmarks, pose.flatten()))\n",
        "\n",
        "  landmarks.flatten()\n",
        "  return landmarks\n"
      ],
      "metadata": {
        "id": "y5BhjIoUIgM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_keypoints(result, color):\n",
        "  keypoint_colors = color * 17\n",
        "  edge_colors = color * 19\n",
        "\n",
        "  image = result.draw(\n",
        "            edge_colors=edge_colors,\n",
        "            joint_thickness=5,\n",
        "            keypoint_colors=keypoint_colors,\n",
        "            keypoint_radius=10,\n",
        "            box_thickness=5,\n",
        "            show_confidence=True,\n",
        "        )\n",
        "  return image"
      ],
      "metadata": {
        "id": "CBHrDh-gPl8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def annotate_image(image, text, confidence, color=(255, 0, 0)):\n",
        "  font = cv2.FONT_HERSHEY_DUPLEX\n",
        "  org1 = (25, 50)\n",
        "  org2 = (25, 100)\n",
        "  fontScale = 1\n",
        "  thickness = 3\n",
        "  image = cv2.putText(image, text, org1, font, fontScale,\n",
        "                 color, thickness, cv2.LINE_AA, False)\n",
        "  image = cv2.putText(image, str(confidence), org2, font, fontScale,\n",
        "                 color, thickness, cv2.LINE_AA, False)\n",
        "\n",
        "  return image"
      ],
      "metadata": {
        "id": "fiqf4_6RQhcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_image(filename, image):\n",
        "  cv2.imwrite(filename, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))"
      ],
      "metadata": {
        "id": "mmDsBiiIQ-uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_video_into_frames(filename):\n",
        "  frames = []\n",
        "  cap = cv2.VideoCapture(filename)\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "  success = 1\n",
        "  while success:\n",
        "    success, frame = cap.read()\n",
        "    if success:\n",
        "      frames.append(frame)\n",
        "\n",
        "  cap.release()\n",
        "  return frames, fps"
      ],
      "metadata": {
        "id": "5_n0l-WSal3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def concat_frames_into_video(filename, frames, fps):\n",
        "  height, width, channels = frames[0].shape\n",
        "\n",
        "  fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "  out = cv2.VideoWriter(filename, fourcc, fps, (width, height))\n",
        "\n",
        "  for frame in frames:\n",
        "    out.write(frame)\n",
        "\n",
        "  out.release()"
      ],
      "metadata": {
        "id": "ZCwlGL91bxvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction_text_color(prediction):\n",
        "  if prediction > 0.5:\n",
        "    color = (0, 255, 0)\n",
        "    text = \"**PIN-DETECTED**\"\n",
        "  elif prediction < 0.5:\n",
        "    color = (255, 0, 0)\n",
        "    text = \"**NO-PIN-DETECTED**\"\n",
        "  return color, text"
      ],
      "metadata": {
        "id": "XPWOdy4gdkX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_on_video(input_filename, output_filename, mlp, confidence):\n",
        "  yolo_nas_pose = init_HPE_model(device)\n",
        "  frames, fps = split_video_into_frames(input_filename)\n",
        "  final_frames = []\n",
        "  for frame in frames:\n",
        "    result = predict_image(yolo_nas_pose, frame, confidence)\n",
        "    landmarks = extract_poses(result)\n",
        "    prediction = predict_position(mlp, landmarks)\n",
        "    color, text = prediction_text_color(prediction)\n",
        "    color = color[::-1] # Reverse color to correct for BGR-RGB\n",
        "    image = draw_keypoints(result, [color])\n",
        "    image = annotate_image(image, text=text, confidence=prediction, color=color)\n",
        "    final_frames.append(image)\n",
        "\n",
        "  concat_frames_into_video(output_filename, final_frames, fps)\n"
      ],
      "metadata": {
        "id": "_L91oIi7cYDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_on_image(input_filename, output_filename, mlp, confidence):\n",
        "  yolo_nas_pose = init_HPE_model(device)\n",
        "  result = predict_image(yolo_nas_pose, input_filename, confidence)\n",
        "  landmarks = extract_poses(result)\n",
        "  prediction = predict_position(mlp, landmarks)\n",
        "  color, text = prediction_text_color(prediction)\n",
        "  image = draw_keypoints(result, [color])\n",
        "  image = annotate_image(image, text=text, confidence=prediction, color=color)\n",
        "  save_image(output_filename, image)"
      ],
      "metadata": {
        "id": "yPpxcVpjEOoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell is for user interaction!"
      ],
      "metadata": {
        "id": "mxQ6ffvfiGpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Modify these variables!\n",
        "input_filename = \"example3.mp4\"\n",
        "output_filename = \"example3_out.mp4\"\n",
        "\n",
        "mlp = load_model()\n",
        "confidence = 0.3\n",
        "\n",
        "predict_on_video(input_filename, output_filename, mlp, confidence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGnw5qZv0QO0",
        "outputId": "746e6e7b-0226-4a62-dbd6-fe256b5feeb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2024-05-01 09:44:40] WARNING - checkpoint_utils.py - :warning: The pre-trained models provided by SuperGradients may have their own licenses or terms and conditions derived from the dataset used for pre-training.\n",
            " It is your responsibility to determine whether you have permission to use the models for your use case.\n",
            " The model you have requested was pre-trained on the coco_pose dataset, published under the following terms: https://cocodataset.org/#termsofuse\n",
            "[2024-05-01 09:44:40] INFO - checkpoint_utils.py - License Notification: YOLO-NAS-POSE pre-trained weights are subjected to the specific license terms and conditions detailed in \n",
            "https://github.com/Deci-AI/super-gradients/blob/master/LICENSE.YOLONAS-POSE.md\n",
            "By downloading the pre-trained weight files you agree to comply with these terms.\n",
            "[2024-05-01 09:44:40] INFO - checkpoint_utils.py - Successfully loaded pretrained weights for architecture yolo_nas_pose_l\n",
            "[2024-05-01 09:44:40] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"
          ]
        }
      ]
    }
  ]
}